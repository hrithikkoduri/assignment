{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Relation Extraction\n",
    "\n",
    "In this assignment, you will work on the [MeasEval](https://competitions.codalab.org/competitions/25770) shared task that was part of SemEval-2021. The goal of **MeasEval** is  the extraction of counts, measurements, and related context from scientific documents. The task is a complex problem that involves solving a number of steps that range from identifying quantities and units of measurement to identify relationships between them. For this assignment, you will focus only on extracting the *HasQuantity* relation:\n",
    "\n",
    " * Given a sentence and the entities it contains, identify the *HasQuantity* relations that link *Quantity* entities with entities of other types. The task can be addressed as Pairwise Relation Extraction.\n",
    "\n",
    "For example, the sentence in the image below contains 1 *Quantity* entity and 2 entities of other types, but only one of the latter is linked to the *Quantity* through the *HasQuantity* relation.\n",
    "\n",
    "![image](img/hasquantity.png)\n",
    "\n",
    "You will develop a Convolutional Neural Network with [keras](https://keras.io/), a high-level Deep Learning API written in **Python** that provides a user-friendly interface for the [TensorFlow](https://www.tensorflow.org/) library, one of the most popular low-level Deep Learning frameworks. You will use the following objects and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Bidirectional, TimeDistributed, Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "When working with Neural Networks, there are a large number of random operations such as initializing the weights of the network, shuffling the data for training, or choosing samples. This causes that different training runs of the same model can lead to different results. To ensure reproducibility, i.e. obtaining the same results in the different runs, the random number generator must be initialized with a fixed value known as seed. In **keras**, this can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "When developing a model, if the results you get are not as expected, try re-initializing the seed by running the cell above before compiling and training the model.\n",
    "\n",
    "> **Note!** With models as complex as Neural Networks, reproducibility is susceptible to factors such as software versions or the hardware on which the models are run. Even with seed initialization, there may be slight differences in the results.\n",
    "\n",
    "Working with Neural Networks also involves defining a number of hyperparameters that set the configuration of the model. Finding the appropriate hyperparameter values requires training the model with different combinations and testing them on the development set. This hyperparameter tuning is a costly process that needs multiple rounds of experimentation. However, for this assignments, you will use the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "maxlen = 130  # Maximum length of the input sequence accepted by the model\n",
    "epochs = 10  # Number of epochs to train the model\n",
    "batch_size = 64  # Number of examples used per gradient update\n",
    "embedding_dim = 300  # Dimension of the embeddings\n",
    "filters = 100  # Number of output filters in the convolution\n",
    "kernel_size = 5  # Length of the convolution window\n",
    "hidden_dim = 10  # Dimension of the hidden dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Training a Deep Learning model with a large train set can be a time-consuming process, as the model needs to iterate over the entire set multiple times, often requiring significant computational resources. During the implementation of the model, it is often a good practice to use only a subset of the training data. This allows a faster debugging of the code. Set the `shrink_dataset` variable as `True` when a faster training is required and set it as `False` to train the model on the whole train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrink_dataset = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Although the value of this variable does not affect the tests that will evaluate your code, the output examples distributed throughout this notebook are based on a `shrink_dataset` variable set as `False`.\n",
    "\n",
    "The train set for the assignment consists of 248 articles with 1366 sentences in total. The test set contains 136 articles with 848 sentences. A development set with 65 documents and 459 sentences is also provided. The dataset includes all the entities annotated at the token level following a BIO schema for 4 different types: *MeasuredEntity*, *MeasuredProperty*, *Qualifier* and *Quantity*. Every entity in the dataset has a unique identifier `annotId`. This identifiers are used to indicate if an entity is linked to a *Quantity* with a *HasQuantity* relation. For example, the annotation for a relation between an \"*involved*\" *MeasuredProperty* and a \"*two beach materials*\" *Quantity* would look as follows:\n",
    "\n",
    "|       | docId                  |   sentId | word        | lemma      | label              | annotId   | rel   |\n",
    "|------:|:-----------------------|---------:|:------------|:-----------|:-------------------|:----------|:------|\n",
    "| 22278 | S0378383912000130-3601 |        3 | involved    | involve    | B-MeasuredProperty | T3-1      | T1-1  |\n",
    "| 22279 | S0378383912000130-3601 |        3 | two         | two        | B-Quantity         | T1-1      | nan   |\n",
    "| 22280 | S0378383912000130-3601 |        3 | beach       | beach      | I-Quantity         | T1-1      | nan   |\n",
    "| 22281 | S0378383912000130-3601 |        3 | materials   | material   | I-Quantity         | T1-1      | nan   |\n",
    "\n",
    "The dataset can be loaded into three `DataFrames` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def load_data(data_path, shrink_dataset, seed):\n",
    "    data = pd.read_csv(data_path, sep=\"\\t\", encoding=\"utf8\")\n",
    "    if shrink_dataset:\n",
    "        sample = data[[\"docId\",  \"sentId\"]].drop_duplicates().sample(frac=0.2, random_state=seed)\n",
    "        data = pd.merge(data, sample, on=[\"docId\", \"sentId\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "train_data = load_data(\"data/train_rels.tsv\", shrink_dataset, seed)\n",
    "dev_data = load_data(\"data/trial_rels.tsv\", shrink_dataset, seed)\n",
    "test_data = load_data(\"data/eval_rels.tsv\", shrink_dataset, seed)\n",
    "train_data[(train_data.docId == \"S0378383912000130-3601\") & (train_data.sentId == 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The `DataFrames` created include the lemmatization of words in the `lemma` columns. You will use the lemmas as the input of the model.\n",
    "\n",
    "> **Note!** The notebook for this assignment provides very little guidance. You are expected to refer to the [documentation](https://keras.io/api/) for details on how to solve the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "In this assignment, you will have to implement some steps to pre-process and obtain a representation of the data. You will implement a model with an `Embedding` lookup table as the input layer, so the tokens of the input sentences should be represented as indexes. Besides, as one would expect, the sentences in the **MeasEval** dataset have different lengths. However, the input for a Deep Learning model is a batch of examples (in this case, sentences) in the form of a single tensor which requires that all examples in the batch must have the same length. Therefore, the sentences should be padded or truncated to a specific length.\n",
    "\n",
    "> **Note!** For this particular task, the `maxlen` value provided to you guarantees that padding is sufficient to make all sentences the same length without the need for truncation. \n",
    "\n",
    "This first of these pre-processing steps will be to obtain both a vocabulary from the train set. The vocabulary should be the list of unique lemmas and must include the special tokens `[PAD]`, that will be used for padding the sequences, and `[UNK]`, that will be used to represent out-of-vocabulary words. The vocabulary must also include the following set of special tokens that will be use later to tag entities:\n",
    "\n",
    "> &lt;MeasuredEntity&gt;, &lt;/MeasuredEntity&gt;, &lt;MeasuredProperty&gt;, &lt;/MeasuredProperty&gt;, &lt;Qualifier&gt;, &lt;/Qualifier&gt;, &lt;Quantity&gt;, &lt;/Quantity&gt;\n",
    "\n",
    "Along with the vocabulary, you will also have to build a dictionary mapping each lemma to its position in the vocabulary. The dictionary will be used later to obtain the representation of the input of the model. The text is already tokenized and lemmatized which will help in this task.  The resulting vocabulary should should include the special tokens `[PAD]` and `[UNK]` in the first two positions and have 5517 items in total:\n",
    "> <pre>\n",
    "Vocabulary size: 5517\n",
    "Vocabulary first 5 words: ['[PAD]', '[UNK]', '&lt;MeasuredEntity&gt;', '&lt;/MeasuredEntity&gt;', '&lt;MeasuredProperty&gt;', '&lt;/MeasuredProperty&gt;', '&lt;Qualifier&gt;', '&lt;/Qualifier&gt;', '&lt;Quantity&gt;', '&lt;/Quantity&gt;', 'datum', 'be', 'draw', 'from', 'the']\n",
    "Vocabulary dictionary: {'[PAD]': 0, '[UNK]': 1, '&lt;MeasuredEntity>': 2, '&lt;/MeasuredEntity&gt;': 3, '&lt;MeasuredProperty&gt;': 4, '&lt;/MeasuredProperty&gt;': 5, '&lt;Qualifier&gt;': 6, '&lt;/Qualifier&gt;': 7, '&lt;Quantity&gt;': 8, '&lt;/Quantity&gt;': 9, 'datum': 10, 'be': 11, 'draw': 12, 'from': 13, 'the': 14}\n",
    "</pre> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "get_vocabulary"
    ]
   },
   "outputs": [],
   "source": [
    "def get_vocabulary(train_data):\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "vocab, word2idx, = get_vocabulary(train_data)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary first 5 words: {vocab[:15]}\")\n",
    "print(f\"Vocabulary dictionary: { {w: word2idx[w] for w in vocab[:15]}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In this assignment, you will follow a pairwise strategy for the extraction of the *HasQuantity* relation. The task must be approached by forming relation candidates by pairing all the *Quantity* entities in a sentence with the rest of entities in the sentence that belong to other entity types. For each of this pairs, the input for the model should be the text of the sentence with the entities of the pair marked using the special tags of their corresponding entity type. For example, the input for the relation candidate involving the *Quantity* \"*involved*\" and the *MeasuredProperty* \"*two beach materials*\" would look like this:\n",
    "\n",
    "> The experiment &lt;MeasuredProperty&gt; involved &lt;/MeasuredProperty&gt; &lt;Quantity&gt; two beach materials &lt;/Quantity&gt; with nominal sediment diameter of 1.5 mm and 8.5 mm.\n",
    "\n",
    "This strategy should result in the following number of relation candidates for each data split:\n",
    "\n",
    ">Number of training relation candidates: 2773  \n",
    "Number of development relation candidates: 797  \n",
    "Number of testing relation candidates: 1445\n",
    "\n",
    "You must reformat the train, development and test `DataFrames` by aggregating the data corresponding to each sentence. The input for the model must be the sequence of lemmas in the sentence and the output a binary True/False label indicating whether the entities of the relation candidate hold a *HasQuantity* relation.  The output of `integrate_sentences` must be a `DataFrame` with a row for each relation candidate and the following columns:\n",
    "\n",
    " * `docId`: the `docId` of the relation candidate\n",
    " * `sentId`: the `sentId` of the relation candidate\n",
    " * `quantityId`: the `annotId` of the *Quantity* entity of the pair\n",
    " * `otherId`: the `annotId` of the other entity of the pair\n",
    " * `lemmas`: the list of the lemmas of the sentence of the relation candidate including the special tags\n",
    " * `label`: a binary label indicating if there is a a *HasQuantity* relation between the entity pair\n",
    " \n",
    "For a sentence with 2 *Quantity* entities and 3 other entities, the `DataFrame` should include 6 rows:\n",
    "\n",
    "|      | docId                  |   sentId | quantityId   | otherId   | lemmas                                                                                                                                                                                                                        | label   |\n",
    "|-----:|:-----------------------|---------:|:-------------|:----------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------|\n",
    "| 1817 | S0378383912000130-3601 |        3 | T1-1         | T4-1      | ['the', '&lt;MeasuredEntity&gt;', 'experiment', '&lt;/MeasuredEntity&gt;', 'involve', '&lt;Quantity&gt;', 'two', 'beach', 'material', '&lt;/Quantity&gt;', 'with', 'nominal', 'sediment', 'diameter', 'of', '1.5', 'mm', 'and', '8.5', 'mm', '.']     | False   |\n",
    "| 1818 | S0378383912000130-3601 |        3 | T1-1         | T3-1      | ['the', 'experiment', '&lt;MeasuredProperty&gt;', 'involve', '&lt;/MeasuredProperty&gt;', '&lt;Quantity&gt;', 'two', 'beach', 'material', '&lt;/Quantity&gt;', 'with', 'nominal', 'sediment', 'diameter', 'of', '1.5', 'mm', 'and', '8.5', 'mm', '.'] | True    |\n",
    "| 1819 | S0378383912000130-3601 |        3 | T1-1         | T3-2      | ['the', 'experiment', 'involve', '&lt;Quantity>', 'two', 'beach', 'material', '&lt;/Quantity&gt;', 'with', '&lt;MeasuredProperty&gt;', 'nominal', 'sediment', 'diameter', '&lt;/MeasuredProperty>', 'of', '1.5', 'mm', 'and', '8.5', 'mm', '.'] | False   |\n",
    "| 1820 | S0378383912000130-3601 |        3 | T1-2         | T4-1      | ['the', '&lt;MeasuredEntity&gt;', 'experiment', '&lt;/MeasuredEntity&gt;', 'involve', 'two', 'beach', 'material', 'with', 'nominal', 'sediment', 'diameter', 'of', '&lt;Quantity>', '1.5', 'mm', 'and', '8.5', 'mm', '&lt;/Quantity&gt;', '.']     | False   |\n",
    "| 1821 | S0378383912000130-3601 |        3 | T1-2         | T3-1      | ['the', 'experiment', '&lt;MeasuredProperty&gt;', 'involve', '&lt;/MeasuredProperty&gt;', 'two', 'beach', 'material', 'with', 'nominal', 'sediment', 'diameter', 'of', '&lt;Quantity>', '1.5', 'mm', 'and', '8.5', 'mm', '&lt;/Quantity&gt;', '.'] | False   |\n",
    "| 1822 | S0378383912000130-3601 |        3 | T1-2         | T3-2      | ['the', 'experiment', 'involve', 'two', 'beach', 'material', 'with', '&lt;MeasuredProperty&gt;', 'nominal', 'sediment', 'diameter', '&lt;/MeasuredProperty&gt;', 'of', '&lt;Quantity&gt;', '1.5', 'mm', 'and', '8.5', 'mm', '&lt;/Quantity&gt;', '.'] | True    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sentence_integrate"
    ]
   },
   "outputs": [],
   "source": [
    "def integrate_sentences(data):\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "train_examples = integrate_sentences(train_data)\n",
    "dev_examples = integrate_sentences(dev_data)\n",
    "test_examples = integrate_sentences(test_data)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(train_examples[(train_examples.docId == \"S0378383912000130-3601\") & (train_examples.sentId == 3)])\n",
    "print(\"Number of training relation candidates: %s\" % len(train_examples))\n",
    "print(\"Number of development relation candidates: %s\" % len(dev_examples))\n",
    "print(\"Number of testing relation candidates: %s\" % len(test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The dataset is now ready for you to get the numerical representation of the input. You must perform two steps to process the sequence of lemmas:\n",
    "1. For each sentence, translate each lemma or label to its corresponding index using the `word2idx` and `label2idx` dictionaries. In case the lemma is not found in `word2idx`, use the index of the `[UNK]` token instead.\n",
    "2. Pad both the sequences of lemmas and the sequences of labels to the same length as defined by the `maxlen` variable using the index of the `[PAD]` token in the vocabulary.   \n",
    "\n",
    "You must also return the target labels in a separate array.\n",
    "\n",
    "Applying `format_examples` to the train, development and test sets should result on 6 arrays with the following shapes:\n",
    "\n",
    ">Shape of training input data:  (2773, 130)  \n",
    "Shape of training output data:  (2773,)  \n",
    "Shape of development input data:  (797, 130)  \n",
    "Shape of development output data:  (797,)  \n",
    "Shape of testing input data:  (1445, 130)  \n",
    "Shape of testing output data:  (1445,)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "format_examples"
    ]
   },
   "outputs": [],
   "source": [
    "def format_examples(data, word2idx, maxlen):\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "x_train, y_train = format_examples(train_examples, word2idx, maxlen)\n",
    "x_dev, y_dev = format_examples(dev_examples, word2idx, maxlen)\n",
    "x_test, y_test = format_examples(test_examples, word2idx, maxlen)\n",
    "print(\"Shape of training input data: \", x_train.shape)\n",
    "print(\"Shape of training output data: \", y_train.shape)\n",
    "print(\"Shape of development input data: \", x_dev.shape)\n",
    "print(\"Shape of development output data: \", y_dev.shape)\n",
    "print(\"Shape of testing input data: \", x_test.shape)\n",
    "print(\"Shape of testing output data: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "You will construct a CNN as a **keras** `Sequential` model with the following 5 layers:\n",
    "\n",
    "1. An `Input` layer where the length of the input sequences is set to `maxlen`.\n",
    "1. An `Embedding` layer with an input dimension equal to the vocabulary size and an embedding dimension defined by `embedding_dim`.\n",
    "2. A `Conv1D` layer with a number of output filters determined by `filters`, a length of convolution window equal to `kernel_size` and a `relu` activation function.\n",
    "3. A `GlobalMaxPooling1D` to downsample the output of the convolutional layer.\n",
    "4. A `Dense` layer with a number of units equal to `hidden_dim` and a `relu` activation function.\n",
    "5. A `Dense` layer with 1 unit and a `sigmoid` activation function.\n",
    "\n",
    "Any option not mentioned in the description should be kept with its default value. The summary of the resulting model should look like:\n",
    "\n",
    "\n",
    "> <pre>\n",
    "> Model: \"sequential\"\n",
    "> __________________________________________________________________________________________\n",
    ">  Layer (type)                           Output Shape                        Param #       \n",
    "> ==========================================================================================\n",
    ">  embedding (Embedding)                  (None, 130, 300)                    1655100       \n",
    ">                                                                                           \n",
    ">  conv1d (Conv1D)                        (None, 126, 100)                    150100        \n",
    ">                                                                                           \n",
    ">  global_max_pooling1d (GlobalMaxPooling  (None, 100)                        0             \n",
    ">  1D)                                                                                      \n",
    ">                                                                                           \n",
    ">  dense (Dense)                          (None, 10)                          1010          \n",
    ">                                                                                           \n",
    ">  dense_1 (Dense)                        (None, 1)                           11            \n",
    ">                                                                                           \n",
    "> ==========================================================================================\n",
    "> Total params: 1,806,221\n",
    "> Trainable params: 1,806,221\n",
    "> Non-trainable params: 0\n",
    "> __________________________________________________________________________________________\n",
    "> </pre>\n",
    "\n",
    "Before returning the model, you should compile it using `'binary_crossentropy'` as the loss function, `'adam'` as the optimizer and `'binary_accuracy'` as a metric to evaluate the model during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create_model"
    ]
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_size, maxlen, embedding_dim, filters, kernel_size, hidden_dim):\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "model = create_model(vocab_size, maxlen, embedding_dim, filters, kernel_size, hidden_dim)\n",
    "model.summary(line_length=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Once the data has been processed and the model has been compiled, you can proceed to train it using the train input and output obtained by `format_examples` as well as the development input and output produced by the same function. Yould should also use the `batch_size` and `epochs` hyperparameters and evaluate the loss and any model metrics on the development data during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "train_model"
    ]
   },
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_dev, y_dev, batch_size, epochs):\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "train_model(model, x_train, y_train, x_dev, y_dev, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "After training, you can apply the model to make predictions on  the test input data produced by `format_examples` using the `batch_size` hyperparameter. Since the output layer of the model is a `sigmoid` function, it will return values ranging from 0 to 1. For each prediction, you should obtain a boolean `True` label in case the prediction is higher than `0.5` or `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "make_predictions"
    ]
   },
   "outputs": [],
   "source": [
    "def make_predictions(model, x_test, batch_size):\n",
    "    #\n",
    "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
    "    #\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "test_examples['prediction'] = make_predictions(model, x_test, batch_size)\n",
    "test_examples[(test_examples.docId == \"S0038071711004354-1624\") & (test_examples.sentId == 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Since the goal of the task is the extraction of the *HasQuality* relation, the metric used for evaluation should only report results for the `True` cases. For this, `binary f1` can be used. For the model trained above, the result of this evaluation should look like:\n",
    "\n",
    "> binary f1: 0.771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    print(\"binary f1: %.3f\" % f1_score(data['label'], data['prediction'], average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "evaluate(test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
